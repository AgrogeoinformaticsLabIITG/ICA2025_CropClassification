{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68294b6a-5a46-4d4f-841b-91b13d0dc80c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "from IPython import display\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models, callbacks, applications\n",
    "from tensorflow.keras import backend as K\n",
    "from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.linalg import sqrtm\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.utils import plot_model\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import rasterio\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cdd708e-3464-4955-9433-951c77474171",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "    # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            logical_gpus = tf.config.list_logical_devices('GPU')\n",
    "            print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "003fc20e-9925-4661-a178-60f1d09397fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SHAPE = (3,8,8,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a6b0c-1292-4983-b31e-b3c75d073e59",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_temporal_data(base_dir, patches, days):\n",
    "    \"\"\"\n",
    "    Loads NPZ files from multiple temporal patches and combines them into a single NumPy array.\n",
    "    \n",
    "    Parameters:\n",
    "        base_dir (str): Base directory containing the temporal data.\n",
    "        patches (list): List of patch names (e.g., ['F440', 'F450']).\n",
    "        days (list): List of day folder names corresponding to different timestamps.\n",
    "    \n",
    "    Returns:\n",
    "        np.ndarray: Combined dataset containing all temporal data for winter_barley.\n",
    "    \"\"\"\n",
    "    winter_barley_data = []\n",
    "    \n",
    "    for patch in patches:\n",
    "        segment_groups = {}\n",
    "\n",
    "        # Iterate over all days\n",
    "        for day_idx, day in enumerate(days):\n",
    "            patch_dir = os.path.join(base_dir, patch, day)\n",
    "            \n",
    "            if not os.path.exists(patch_dir):\n",
    "                print(f\"Warning: Directory {patch_dir} does not exist. Skipping...\")\n",
    "                continue\n",
    "            \n",
    "            # Process files in each day's folder\n",
    "            for filename in os.listdir(patch_dir):\n",
    "                if filename.endswith(\".npz\"):\n",
    "                    match = re.search(r'_(\\d+)_segment_(\\d+)_', filename)  # Extract segment ID\n",
    "                    if match:\n",
    "                        segment_id = match.group(2)\n",
    "                        if segment_id not in segment_groups:\n",
    "                            segment_groups[segment_id] = {}\n",
    "                        segment_groups[segment_id][day_idx] = os.path.join(patch_dir, filename)\n",
    "        \n",
    "        # Process only segments that exist in all three days\n",
    "        for segment_id, files in segment_groups.items():\n",
    "            if len(files) == len(days):  # Ensure we have all three days\n",
    "                segment_data = []\n",
    "\n",
    "                # Load NPZ files for the segment\n",
    "                for day_idx in range(len(days)):\n",
    "                    data = np.load(files[day_idx])\n",
    "                    image = data['image']  # Assuming the key is 'image'\n",
    "                    segment_data.append(image)\n",
    "                \n",
    "                # Stack along the first axis to maintain temporal ordering\n",
    "                segment_data = np.array(segment_data)  # Shape: (3, H, W, C)\n",
    "                winter_barley_data.append(segment_data)\n",
    "    \n",
    "    # Convert the collected data into a NumPy array\n",
    "    if winter_barley_data:\n",
    "        winter_barley_array = np.array(winter_barley_data)  # Shape: (num_segments, 3, H, W, C)\n",
    "        return winter_barley_array\n",
    "    else:\n",
    "        print(\"No valid temporal data found.\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "base_directory = \"dataset/segmented_npz/processed_segmented_npz/_8x8/temporal/c11_c22/winter_barley/\"\n",
    "patches = [\"F440\", \"F450\"]\n",
    "days = [\"_0209\", \"_0912\", \"_1609\"]  # Replace with actual day folder names\n",
    "\n",
    "winter_barley_array = load_temporal_data(base_directory, patches, days)\n",
    "\n",
    "# Example usage\n",
    "base_directory = \"dataset/segmented_npz/processed_segmented_npz/_8x8/temporal/c11_c22/winter_wheat/\"\n",
    "patches = [\"F230\", \"F250\"]\n",
    "days = [\"_0209\", \"_0912\", \"_1609\"]  # Replace with actual day folder names\n",
    "\n",
    "winter_wheat_array = load_temporal_data(base_directory, patches, days)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ba052aa-4c3d-4ab5-936d-dc6a241b0f56",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_barley_y = [0 for i in range(len(winter_barley_array))]\n",
    "winter_wheat_y = [1 for i in range(len(winter_wheat_array))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ddfdf4-22c9-4af2-9428-de81b4d8e051",
   "metadata": {},
   "outputs": [],
   "source": [
    "winter_barley_array1, winter_barley_test, winter_barley_y1, winter_barley_ytest = train_test_split(winter_barley_array, winter_barley_y, test_size=0.2, random_state=7)\n",
    "winter_wheat_array1, winter_wheat_test, winter_wheat_y1, winter_wheat_ytest = train_test_split(winter_wheat_array, winter_wheat_y, test_size=0.2, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e4b6528-97d0-4c58-80dd-fc7d0cbe4236",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtest = np.concatenate([winter_barley_test, winter_wheat_test], axis=0)\n",
    "ytest = np.concatenate([winter_barley_ytest, winter_wheat_ytest], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebfb70d-ac84-4407-8fd2-118c925bb85c",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_barley, xval_barley, ytrain_barley, yval_barley = train_test_split(winter_barley_array1, winter_barley_y1, test_size=0.1, random_state=7)\n",
    "xtrain_wheat, xval_wheat, ytrain_wheat, yval_wheat = train_test_split(winter_wheat_array1, winter_wheat_y1, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b8805d3-61e2-4007-af05-1dc20822aa4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain = np.concatenate([xtrain_barley, xtrain_wheat], axis=0)\n",
    "ytrain = np.concatenate([ytrain_barley, ytrain_wheat], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45a51419-71bd-4f29-99b4-4ebf3808edf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "xval = np.concatenate([xval_barley, xval_wheat], axis=0)\n",
    "yval = np.concatenate([yval_barley, yval_wheat], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53999299-b9a3-4860-9bfd-5de75600144d",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.bincount(ytrain)[1]-np.bincount(ytrain)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc20d64-8118-4017-bb25-d2d178f147da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize ImageDataGenerator with transformations\n",
    "datagen = ImageDataGenerator(\n",
    "    featurewise_center=False,  \n",
    "    samplewise_center=False,  \n",
    "    featurewise_std_normalization=False,  \n",
    "    samplewise_std_normalization=False,  \n",
    "    rotation_range=25,  \n",
    "    width_shift_range=0.1,  \n",
    "    height_shift_range=0.1,  \n",
    "    horizontal_flip=False,  \n",
    "    vertical_flip=False\n",
    ")\n",
    "\n",
    "# Reshape xtrain to (num_samples * 3, 8, 8, 3) so that it's 4D\n",
    "num_samples = xtrain.shape[0]  # Original number of samples\n",
    "xtrain_reshaped = xtrain.reshape(-1, 8, 8, 3)  # New shape: (num_samples * 3, 8, 8, 3)\n",
    "\n",
    "# Fit the generator\n",
    "datagen.fit(xtrain_reshaped)\n",
    "\n",
    "# Generate augmented images\n",
    "samples = datagen.flow(xtrain_reshaped, batch_size=1)\n",
    "\n",
    "# Store the augmented images and reshape them back to (num_samples, 3, 8, 8, 3)\n",
    "image = []\n",
    "for i in range(1159 * 3):  # Iterate over each sample\n",
    "    img = next(samples).squeeze()  # Remove batch dimension\n",
    "    image.append(img)\n",
    "\n",
    "# Convert list to numpy array and reshape back to original form\n",
    "image = np.array(image).reshape(-1, 3, 8, 8, 3)  # Final shape: (num_samples, 3, 8, 8, 3)\n",
    "\n",
    "print(\"Augmented Image Shape:\", image.shape)  # Expected: (num_samples, 3, 8, 8, 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9fe1497-4cfd-4939-a7b7-8688dc3bf748",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_augmented = np.concatenate((xtrain,image),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "977dc19e-62ae-44da-8911-2a86f76b3bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "ytrain1 = [0 for i in range(1159)]\n",
    "ytrain_augmented = np.concatenate((ytrain,ytrain1),axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4633cb96-a929-415a-bf98-c84b8ac0a7aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.concatenate((xtrain_augmented,xval,xtest), axis=0)\n",
    "y = np.concatenate((ytrain_augmented,yval,ytest), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fb5be5e-a471-47dd-a7b8-f7a70873fd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score_normalization(data):\n",
    "    mean = np.mean(data, axis=(1, 2), keepdims=True)  # Compute mean per channel\n",
    "    std = np.std(data, axis=(1, 2), keepdims=True)  # Compute std per channel\n",
    "    return (data - mean) / (std + 1e-7)  # Normalize with small epsilon to avoid division by zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "795d8843-ec9e-4454-b92b-5cff9afbc13a",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_normalized = z_score_normalization(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3e60bf0-b53f-41d8-962a-f09f34406741",
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_normalized, xtest_normalized, ytrain_augmented, ytest = train_test_split(x_normalized,y, test_size=0.2, random_state=7)\n",
    "xtrain_normalized, xval_normalized, ytrain_augmented, yval = train_test_split(xtrain_normalized,ytrain_augmented, test_size=0.1, random_state=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d890baf7-b023-4065-aa5d-9bca80d7647e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn_model = models.Sequential()\n",
    "\n",
    "# 3D Convolutional Layers\n",
    "cnn_model.add(layers.Conv3D(16, (3,3,3), activation='relu', input_shape=(3,8,8,3), padding='same'))\n",
    "cnn_model.add(layers.BatchNormalization())\n",
    "\n",
    "cnn_model.add(layers.Conv3D(32, (3,3,3), activation='relu', padding='same'))\n",
    "cnn_model.add(layers.BatchNormalization())\n",
    "\n",
    "cnn_model.add(layers.Conv3D(64, (3,3,3), activation='relu', padding='same'))\n",
    "cnn_model.add(layers.BatchNormalization())\n",
    "\n",
    "cnn_model.add(layers.MaxPooling3D(pool_size=(1,2,2)))  # Reduce spatial dimensions\n",
    "\n",
    "cnn_model.add(layers.Conv3D(128, (3,3,3), activation='relu', padding='same'))\n",
    "cnn_model.add(layers.BatchNormalization())\n",
    "\n",
    "cnn_model.add(layers.MaxPooling3D(pool_size=(1,2,2)))  # Further spatial reduction\n",
    "\n",
    "# Flatten and Fully Connected Layers\n",
    "cnn_model.add(layers.Flatten())\n",
    "\n",
    "cnn_model.add(layers.Dense(128, activation='relu'))\n",
    "cnn_model.add(layers.BatchNormalization())\n",
    "cnn_model.add(layers.Dropout(0.3))\n",
    "\n",
    "cnn_model.add(layers.Dense(1, activation='sigmoid'))  # Binary classification\n",
    "\n",
    "# Compile Model\n",
    "cnn_model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Model Summary\n",
    "cnn_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81f6917-2f4f-48ee-b948-d5f83ab6f1a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping_callback = callbacks.EarlyStopping(patience=100)\n",
    "batch_size = 10\n",
    "print(batch_size)\n",
    "\n",
    "cnn_model.compile(optimizer=Adam(learning_rate=0.01), loss=tf.keras.losses.BinaryCrossentropy(),\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "037d8846-a7c5-404e-9919-7d06c0b0cf00",
   "metadata": {},
   "outputs": [],
   "source": [
    "precision_0 = []\n",
    "precision_1 = []\n",
    "recall_0 = []\n",
    "recall_1 = []\n",
    "accuracy = []\n",
    "auc_score = []\n",
    "class TestAfterEpoch(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, test_data, test_labels):\n",
    "        self.test_data = test_data\n",
    "        self.test_labels = test_labels\n",
    "        \n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        self.model.evaluate(self.test_data, self.test_labels)\n",
    "        predictions = self.model.predict(self.test_data)\n",
    "        y_pred_classes = np.where(predictions > 0.5, 1,0)\n",
    "        \n",
    "        cm = confusion_matrix(self.test_labels, y_pred_classes)\n",
    "        print(cm)\n",
    "        \n",
    "        labels = ['Class 0', 'Class 1']  # Optional: class labels\n",
    "\n",
    "        disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "        disp.plot(cmap='Blues', values_format='d')  # Customize color and format\n",
    "\n",
    "        # Customize plot appearance\n",
    "        plt.title('Confusion Matrix', fontsize=16)\n",
    "        plt.xlabel('Predicted Labels', fontsize=14)\n",
    "        plt.ylabel('True Labels', fontsize=14)\n",
    "        plt.xticks(fontsize=12)\n",
    "        plt.yticks(fontsize=12)\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "        \n",
    "        print(\"\\n\\n\")\n",
    "        \n",
    "        report = classification_report(self.test_labels, y_pred_classes, digits=5, output_dict=True)\n",
    "        precision_0.append(report[\"0\"][\"precision\"])\n",
    "        precision_1.append(report[\"1\"][\"precision\"])\n",
    "        recall_0.append(report[\"0\"][\"recall\"])\n",
    "        recall_1.append(report[\"1\"][\"recall\"])\n",
    "        accuracy.append(report[\"accuracy\"])\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        print(classification_report(self.test_labels, y_pred_classes, digits=5))\n",
    "        \n",
    "        # Assuming you have y_true (true labels) and y_pred_proba (predicted probabilities)\n",
    "        y_true = self.test_labels  # True labels\n",
    "        y_pred_proba = predictions  # Predicted probabilities for the positive class\n",
    "\n",
    "        # Calculate ROC curve\n",
    "        fpr, tpr, thresholds = roc_curve(y_true, y_pred_proba)\n",
    "\n",
    "        # Calculate AUC score\n",
    "        auc = roc_auc_score(y_true, y_pred_proba)\n",
    "        auc_score.append(auc)\n",
    "\n",
    "        with open(\"3d_cnn_3_channel.txt\", \"a\") as file:\n",
    "            file.write(f\"fpr: {fpr}\")\n",
    "            file.write(f\"\\n tpr: {tpr}\")\n",
    "            file.write(f\"\\n auc: {auc:.4f}\")\n",
    "\n",
    "        # Plot ROC curve\n",
    "        plt.figure()\n",
    "        plt.plot(fpr, tpr, label=f'AUC = {auc:.4f}')\n",
    "        plt.plot([0, 1], [0, 1], 'k--')  # Diagonal line for random classifier\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate')\n",
    "        plt.title('ROC Curve')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "853bab84-1fbc-4b55-972c-3b4654e466a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_callback = TestAfterEpoch(xtest_normalized, ytest)\n",
    "reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, min_lr=1e-3)\n",
    "epochs_info = cnn_model.fit(xtrain_normalized,\n",
    "                            ytrain_augmented,\n",
    "                            epochs=500,\n",
    "                           validation_data=(xval_normalized, yval),\n",
    "                           callbacks = [test_callback, reduce_lr])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
